{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c51594ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db53734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure the clt package is discoverable\n",
    "project_dir = os.path.abspath('..')\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)\n",
    "\n",
    "from clt.activation_generation.generator import ActivationConfig\n",
    "from clt.nnsight.extractor import ActivationExtractorCLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9464384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:clt.config.data_config:ActivationConfig Summary:\n",
      "  Model: allenai/OLMo-2-0425-1B-Instruct\n",
      "  Dataset: allenai/olmo-mix-1124 (Split: train, Skip: None)\n",
      "  Target Tokens: 1000000\n",
      "  Chunk Threshold: 1\n",
      "  Activation Dtype: bfloat16\n",
      "  Output Dir: None\n"
     ]
    }
   ],
   "source": [
    "cfg = ActivationConfig(\n",
    "    model_name=\"allenai/OLMo-2-0425-1B-Instruct\",\n",
    "    mlp_input_module_path_template=\"model.layers.{}.mlp.input\",\n",
    "    mlp_output_module_path_template=\"model.layers.{}.mlp.output\",\n",
    "    # model_dtype=args.model_dtype,\n",
    "    activation_dtype=\"bfloat16\",\n",
    "    dataset_path=\"allenai/olmo-mix-1124\",\n",
    "    # dataset_split=args.dataset_split,\n",
    "    # dataset_text_column=args.dataset_text_column,\n",
    "    # dataset_skip=args.dataset_skip,\n",
    "    context_size=4096,\n",
    "    inference_batch_size=8,\n",
    "    # exclude_special_tokens=args.exclude_special_tokens,\n",
    "    prepend_bos=True,\n",
    "    # streaming=args.streaming,\n",
    "    # dataset_trust_remote_code=args.trust_remote_code,\n",
    "    # cache_path=args.cache_path,\n",
    "    target_total_tokens=1000000,\n",
    "    activation_dir=None,\n",
    "    # output_format=args.output_format,\n",
    "    compression=None,\n",
    "    chunk_token_threshold=1,\n",
    "    compute_norm_stats=True,\n",
    "    # nnsight_tracer_kwargs=nnsight_tracer_kwargs,\n",
    "    # nnsight_invoker_args=nnsight_invoker_args,\n",
    "    # remote_server_url=args.remote_server_url,\n",
    "    # delete_after_upload=args.delete_after_upload,\n",
    "    # upload_max_retries=args.upload_max_retries,\n",
    "    # upload_initial_backoff=args.upload_initial_backoff,\n",
    "    # upload_max_backoff=args.upload_max_backoff,\n",
    "    # enable_profiling=args.enable_profiling,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "489a817f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ActivationExtractorCLT.stream_activations at 0xcec7200>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "extractor = ActivationExtractorCLT(\n",
    "    model_name=cfg.model_name,\n",
    "    mlp_input_module_path_template=cfg.mlp_input_module_path_template,\n",
    "    mlp_output_module_path_template=cfg.mlp_output_module_path_template,\n",
    "    device=device,\n",
    "    model_dtype=cfg.model_dtype,\n",
    "    context_size=cfg.context_size,\n",
    "    inference_batch_size=cfg.inference_batch_size,\n",
    "    exclude_special_tokens=cfg.exclude_special_tokens,\n",
    "    prepend_bos=cfg.prepend_bos,\n",
    "    nnsight_tracer_kwargs=cfg.nnsight_tracer_kwargs,\n",
    "    nnsight_invoker_args=cfg.nnsight_invoker_args\n",
    ")\n",
    "stream = extractor.stream_activations(\n",
    "    dataset_path=cfg.dataset_path,\n",
    "    dataset_split=cfg.dataset_split,\n",
    "    dataset_text_column=cfg.dataset_text_column,\n",
    "    dataset_skip=cfg.dataset_skip,\n",
    "    streaming=cfg.streaming,\n",
    "    dataset_trust_remote_code=cfg.dataset_trust_remote_code,\n",
    "    cache_path=cfg.cache_path,\n",
    ")\n",
    "stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1558410f",
   "metadata": {},
   "source": [
    "# Generate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26a94540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import queue\n",
    "import random\n",
    "import logging\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any, DefaultDict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from urllib.parse import quote, urljoin\n",
    "\n",
    "# ––– local imports (keep relative to package root) –––\n",
    "from clt.nnsight.extractor import ActivationExtractorCLT  # noqa: E402\n",
    "from clt.config.data_config import ActivationConfig  # noqa: E402\n",
    "\n",
    "# --- Profiling Imports ---\n",
    "import time  # Keep this one\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "\n",
    "# Local application imports\n",
    "# from clt.training.utils import torch_bfloat16_to_numpy_uint16 # Removed unused import\n",
    "\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    GPUtil = None  # type: ignore\n",
    "# --- End Profiling Imports ---\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "ActivationBatch = Tuple[Dict[int, torch.Tensor], Dict[int, torch.Tensor]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd3cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clt.activation_generation.generator import _RunningStat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77fe5b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?tok/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  2%|▏         | 15001/1000000 [00:11<12:23, 1325.58tok/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers=%d d_model=%d dtype=%s 16 2048 torch.float32\n"
     ]
    }
   ],
   "source": [
    "tgt_tokens = cfg.target_total_tokens\n",
    "chunk_tokens = cfg.chunk_token_threshold\n",
    "pbar = tqdm(total=tgt_tokens or None, unit=\"tok\", smoothing=0.2)\n",
    "\n",
    "# Collect manifest rows in‑memory to avoid pre‑allocation mismatch bugs.\n",
    "# Each entry is (chunk_id, local_row).  For 1 M tokens this is only 8 MB.\n",
    "manifest_rows: List[np.ndarray] = []\n",
    "\n",
    "# Norm‑stat structures\n",
    "stats: Dict[int, Dict[str, _RunningStat]] = {}\n",
    "\n",
    "g_row = 0\n",
    "c_idx = 0\n",
    "buf_inp: Dict[int, List[torch.Tensor]] = {}\n",
    "buf_tgt: Dict[int, List[torch.Tensor]] = {}\n",
    "layer_ids: Optional[List[int]] = None\n",
    "d_model = -1\n",
    "dtype_str = \"unknown\"\n",
    "\n",
    "for batch_idx, (batch_inp, batch_tgt) in enumerate(stream):\n",
    "    # with self._conditional_measure(\"batch_processing_total\"):\n",
    "    if tgt_tokens and g_row >= tgt_tokens:\n",
    "        break\n",
    "    if not batch_inp:\n",
    "        continue\n",
    "\n",
    "    # with self._conditional_measure(\"batch_metadata_setup\"):\n",
    "    if layer_ids is None:\n",
    "        layer_ids = sorted(batch_inp.keys())\n",
    "        d_model = batch_inp[layer_ids[0]].shape[-1]\n",
    "        dtype_str = str(batch_inp[layer_ids[0]].dtype)\n",
    "        # if self.profiler:\n",
    "        #     self.profiler.set_layer_ids_ref(layer_ids)\n",
    "        for lid in layer_ids:\n",
    "            buf_inp[lid] = []\n",
    "            buf_tgt[lid] = []\n",
    "            if cfg.compute_norm_stats:\n",
    "                stats[lid] = {\n",
    "                    \"inputs\": _RunningStat(d_model, device=device),\n",
    "                    \"targets\": _RunningStat(d_model, device=device),\n",
    "                }\n",
    "        print(\n",
    "            \"Layers=%d d_model=%d dtype=%s\", len(layer_ids) if layer_ids else 0, d_model, dtype_str\n",
    "        )\n",
    "\n",
    "    n_tok_in_batch = 0\n",
    "    if layer_ids and batch_inp.get(layer_ids[0]) is not None:\n",
    "        n_tok_in_batch = batch_inp[layer_ids[0]].shape[0]\n",
    "\n",
    "    # with self._conditional_measure(\"batch_gpu_tensor_accumulate\"):\n",
    "    if layer_ids:\n",
    "        for lid in layer_ids:\n",
    "            if lid in batch_inp and lid in batch_tgt:\n",
    "                inp = batch_inp[lid].detach()\n",
    "                tgt = batch_tgt[lid].detach()\n",
    "                buf_inp[lid].append(inp)\n",
    "                buf_tgt[lid].append(tgt)\n",
    "                if cfg.compute_norm_stats and lid in stats:\n",
    "                    # with self._conditional_measure(f\"batch_norm_stats_update_layer_{lid}\"):\n",
    "                    stats[lid][\"inputs\"].update(inp)\n",
    "                    stats[lid][\"targets\"].update(tgt)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Layer {lid} expected but not found in current batch. Skipping accumulation for this layer.\"\n",
    "                )\n",
    "\n",
    "    if n_tok_in_batch > 0:\n",
    "        g_row += n_tok_in_batch\n",
    "        pbar.update(n_tok_in_batch)\n",
    "    #     if self.profiler:\n",
    "    #         self.profiler.total_tokens_processed_for_batch_profiling += n_tok_in_batch\n",
    "    # if self.profiler:\n",
    "    #     self.profiler.batch_processing_total_calls += 1\n",
    "\n",
    "    # if layer_ids and buf_inp.get(layer_ids[0]):\n",
    "    #     cur_rows = sum(t.shape[0] for t in buf_inp[layer_ids[0]])\n",
    "    #     if cur_rows >= chunk_tokens:\n",
    "    #         # with self._conditional_measure(\"chunk_write_dispatch\"):\n",
    "    #         self._write_chunk(\n",
    "    #             c_idx,\n",
    "    #             buf_inp,\n",
    "    #             buf_tgt,\n",
    "    #             layer_ids,\n",
    "    #             d_model,\n",
    "    #             cur_rows,\n",
    "    #             manifest_rows,\n",
    "    #             g_row - cur_rows,\n",
    "    #         )\n",
    "    #         c_idx += 1\n",
    "    #         # with self._conditional_measure(\"chunk_buffer_clear\"):\n",
    "    #         if layer_ids:\n",
    "    #             for lid_clear in layer_ids:\n",
    "    #                 buf_inp[lid_clear].clear()\n",
    "    #                 buf_tgt[lid_clear].clear()\n",
    "    # if batch_idx > 0 and batch_idx % 50 == 0:\n",
    "    #     if self.profiler:\n",
    "    #         self.profiler.log_system_metrics(f\"batch_interval_{batch_idx}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880ea600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]),\n",
       " dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf_inp.keys(), buf_tgt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c895f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15001, 2048])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf_inp[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3797e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15001, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buf_tgt[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13c742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15001, 2048])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_inp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58999648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save():\n",
    "    tgt_tokens = cfg.target_total_tokens\n",
    "    chunk_tokens = cfg.chunk_token_threshold\n",
    "    pbar = tqdm(total=tgt_tokens or None, unit=\"tok\", smoothing=0.2)\n",
    "\n",
    "    # Collect manifest rows in‑memory to avoid pre‑allocation mismatch bugs.\n",
    "    # Each entry is (chunk_id, local_row).  For 1 M tokens this is only 8 MB.\n",
    "    manifest_rows: List[np.ndarray] = []\n",
    "\n",
    "    # Norm‑stat structures\n",
    "    stats: Dict[int, Dict[str, _RunningStat]] = {}\n",
    "\n",
    "    g_row = 0\n",
    "    c_idx = 0\n",
    "    buf_inp: Dict[int, List[torch.Tensor]] = {}\n",
    "    buf_tgt: Dict[int, List[torch.Tensor]] = {}\n",
    "    layer_ids: Optional[List[int]] = None\n",
    "    d_model = -1\n",
    "    dtype_str = \"unknown\"\n",
    "\n",
    "    for batch_idx, (batch_inp, batch_tgt) in enumerate(stream):\n",
    "        # with self._conditional_measure(\"batch_processing_total\"):\n",
    "        if tgt_tokens and g_row >= tgt_tokens:\n",
    "            break\n",
    "        if not batch_inp:\n",
    "            continue\n",
    "\n",
    "        # with self._conditional_measure(\"batch_metadata_setup\"):\n",
    "        if layer_ids is None:\n",
    "            layer_ids = sorted(batch_inp.keys())\n",
    "            d_model = batch_inp[layer_ids[0]].shape[-1]\n",
    "            dtype_str = str(batch_inp[layer_ids[0]].dtype)\n",
    "            # if self.profiler:\n",
    "            #     self.profiler.set_layer_ids_ref(layer_ids)\n",
    "            for lid in layer_ids:\n",
    "                buf_inp[lid] = []\n",
    "                buf_tgt[lid] = []\n",
    "                if cfg.compute_norm_stats:\n",
    "                    stats[lid] = {\n",
    "                        \"inputs\": _RunningStat(d_model, device=device),\n",
    "                        \"targets\": _RunningStat(d_model, device=device),\n",
    "                    }\n",
    "            print(\n",
    "                \"Layers=%d d_model=%d dtype=%s\", len(layer_ids) if layer_ids else 0, d_model, dtype_str\n",
    "            )\n",
    "\n",
    "        n_tok_in_batch = 0\n",
    "        if layer_ids and batch_inp.get(layer_ids[0]) is not None:\n",
    "            n_tok_in_batch = batch_inp[layer_ids[0]].shape[0]\n",
    "\n",
    "        # with self._conditional_measure(\"batch_gpu_tensor_accumulate\"):\n",
    "        if layer_ids:\n",
    "            for lid in layer_ids:\n",
    "                if lid in batch_inp and lid in batch_tgt:\n",
    "                    inp = batch_inp[lid].detach()\n",
    "                    tgt = batch_tgt[lid].detach()\n",
    "                    buf_inp[lid].append(inp)\n",
    "                    buf_tgt[lid].append(tgt)\n",
    "                    if cfg.compute_norm_stats and lid in stats:\n",
    "                        # with self._conditional_measure(f\"batch_norm_stats_update_layer_{lid}\"):\n",
    "                        stats[lid][\"inputs\"].update(inp)\n",
    "                        stats[lid][\"targets\"].update(tgt)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Layer {lid} expected but not found in current batch. Skipping accumulation for this layer.\"\n",
    "                    )\n",
    "\n",
    "        if n_tok_in_batch > 0:\n",
    "            g_row += n_tok_in_batch\n",
    "            pbar.update(n_tok_in_batch)\n",
    "        #     if self.profiler:\n",
    "        #         self.profiler.total_tokens_processed_for_batch_profiling += n_tok_in_batch\n",
    "        # if self.profiler:\n",
    "        #     self.profiler.batch_processing_total_calls += 1\n",
    "\n",
    "        if layer_ids and buf_inp.get(layer_ids[0]):\n",
    "            cur_rows = sum(t.shape[0] for t in buf_inp[layer_ids[0]])\n",
    "            if cur_rows >= chunk_tokens:\n",
    "                # with self._conditional_measure(\"chunk_write_dispatch\"):\n",
    "                self._write_chunk(\n",
    "                    c_idx,\n",
    "                    buf_inp,\n",
    "                    buf_tgt,\n",
    "                    layer_ids,\n",
    "                    d_model,\n",
    "                    cur_rows,\n",
    "                    manifest_rows,\n",
    "                    g_row - cur_rows,\n",
    "                )\n",
    "                c_idx += 1\n",
    "                # with self._conditional_measure(\"chunk_buffer_clear\"):\n",
    "                if layer_ids:\n",
    "                    for lid_clear in layer_ids:\n",
    "                        buf_inp[lid_clear].clear()\n",
    "                        buf_tgt[lid_clear].clear()\n",
    "        # if batch_idx > 0 and batch_idx % 50 == 0:\n",
    "        #     if self.profiler:\n",
    "        #         self.profiler.log_system_metrics(f\"batch_interval_{batch_idx}\")\n",
    "\n",
    "    # Flush final partial chunk\n",
    "    if layer_ids and buf_inp.get(layer_ids[0]):\n",
    "        # with self._conditional_measure(\"final_chunk_write_dispatch\"):\n",
    "        rows = sum(t.shape[0] for t in buf_inp[layer_ids[0]])\n",
    "        self._write_chunk(\n",
    "            c_idx,\n",
    "            buf_inp,\n",
    "            buf_tgt,\n",
    "            layer_ids,\n",
    "            d_model,\n",
    "            rows,\n",
    "            manifest_rows,\n",
    "            g_row - rows,\n",
    "        )\n",
    "        c_idx += 1\n",
    "\n",
    "    # if self.profiler:\n",
    "    #     self.profiler.log_system_metrics(\"pre_manifest_write\")\n",
    "    # with self._conditional_measure(\"manifest_concatenate_and_write\"):\n",
    "    if manifest_rows:\n",
    "        manifest_arr = np.concatenate(manifest_rows, axis=0)\n",
    "        manifest_arr.tofile(self.manifest_final)\n",
    "    else:\n",
    "        print(\"Manifest_rows is empty, skipping manifest write.\")\n",
    "\n",
    "    # Upload final manifest if remote\n",
    "    if self.storage_type == \"remote\" and self.manifest_final.exists():\n",
    "        try:\n",
    "            self._upload_binary_file(self.manifest_final, \"manifest\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to upload manifest.bin: %s\", e)\n",
    "\n",
    "    # Write metadata JSON\n",
    "    meta = {\n",
    "        \"model_name\": cfg.model_name,\n",
    "        \"dataset\": cfg.dataset_path,\n",
    "        \"split\": cfg.dataset_split,\n",
    "        \"num_layers\": len(layer_ids or []),\n",
    "        \"d_model\": d_model,\n",
    "        \"dtype\": cfg.activation_dtype,\n",
    "        \"total_tokens\": g_row,\n",
    "        \"chunk_tokens\": chunk_tokens,\n",
    "        \"created\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    # with self._conditional_measure(\"metadata_json_write\"):\n",
    "    with open(self.out_dir / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(\"metadata.json written\")\n",
    "\n",
    "    meta_path = self.out_dir / \"metadata.json\"\n",
    "    if self.storage_type == \"remote\" and self.cfg.remote_server_url:\n",
    "        # with self._conditional_measure(\"metadata_json_upload\"):\n",
    "        try:\n",
    "            self._upload_json(meta_path, \"metadata\")\n",
    "            print(\"metadata.json uploaded to server\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to upload metadata.json: %s\", e)\n",
    "\n",
    "    # Write norm_stats.json\n",
    "    if cfg.compute_norm_stats and stats:\n",
    "        norm: Dict[str, Any] = {}\n",
    "        if layer_ids:\n",
    "            for lid in layer_ids:\n",
    "                if lid in stats:\n",
    "                    # with self._conditional_measure(f\"norm_stats_finalize_layer_{lid}\"):\n",
    "                    m_in, s_in = stats[lid][\"inputs\"].finalize()\n",
    "                    m_tg, s_tg = stats[lid][\"targets\"].finalize()\n",
    "                    norm[str(lid)] = {\n",
    "                        \"inputs\": {\"mean\": m_in.tolist(), \"std\": s_in.tolist()},\n",
    "                        \"targets\": {\"mean\": m_tg.tolist(), \"std\": s_tg.tolist()},\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Layer ID {lid} not found in stats dict during norm_stats finalization.\")\n",
    "        else:\n",
    "            print(\"layer_ids is None, cannot write norm_stats.\")\n",
    "\n",
    "        if norm:\n",
    "            # with self._conditional_measure(\"norm_stats_json_write\"):\n",
    "            with open(self.out_dir / \"norm_stats.json\", \"w\") as f:\n",
    "                json.dump(norm, f)\n",
    "            print(\"norm_stats.json written\")\n",
    "\n",
    "            norm_path = self.out_dir / \"norm_stats.json\"\n",
    "            if self.storage_type == \"remote\" and self.cfg.remote_server_url:\n",
    "                # with self._conditional_measure(\"norm_stats_json_upload\"):\n",
    "                try:\n",
    "                    self._upload_json(norm_path, \"norm_stats\")\n",
    "                    print(\"norm_stats.json uploaded to server\")\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to upload norm_stats.json: %s\", e)\n",
    "        elif cfg.compute_norm_stats:\n",
    "            print(\"Norm stats computation was enabled, but no norm stats were generated.\")\n",
    "\n",
    "    # # Finish uploading (only if we are in remote mode)\n",
    "    # if self.storage_type == \"remote\" and self.uploader and self.upload_q:\n",
    "    #     with self._conditional_measure(\"uploader_join\"):\n",
    "    #         self.upload_q.put(None)\n",
    "    #         self.upload_q.join()\n",
    "    # print(\"Finished: %d chunks, %s tokens\", c_idx, f\"{g_row:,}\")\n",
    "    # if self.profiler:\n",
    "    #     self.profiler.log_system_metrics(\"final_system_state\")\n",
    "    #     self.profiler.report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
